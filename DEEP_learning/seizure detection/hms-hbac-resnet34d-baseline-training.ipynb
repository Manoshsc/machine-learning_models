{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":59093,"databundleVersionId":7469972,"sourceType":"competition"},{"sourceId":7392733,"sourceType":"datasetVersion","datasetId":4297749}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# About\n\nIn this notebook, I'll share an image clacification approach for given spectrograms.  \n\nI tried several experiments, but didn't obtain good results :(\n\n\n* **version 1**: naive approach\n* **version 2**: For comparing with [Chris's EfficientNetB2 Starter](https://www.kaggle.com/code/cdeotte/efficientnetb2-starter-lb-0-57), I added **log transform** and **LR scheduling**.\n\n## Experimental Settings\n\n### model\n* backbone: resnet34d (use the pretrained model provided by [timm](https://github.com/huggingface/pytorch-image-models))\n* head classifier: one linear layer\n* num of input channels: 1\n\n### data augmentationÂ¶\n* implemented by [albumentations](https://albumentations.ai/)\n* Train\n    * Resize\n* Val, Test\n    * Resize\n    \n### learning settings\n* CV Strategy: Stratified Group KFold (K=5)\n    * y: `expert_consensus`\n    * group: `patient_id`\n* max epochs: 9\n* data:\n    * input image size: 1x512x512\n    * batch size: 32\n* loss: [KLDivLoss](https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html)\n* optimizer: AdamW\n    * learning rate: 1.0e-03\n    * weight decay: 1.0e-02\n    \n* lr scheduler: OneCycleLR\n    * max lr: 1.0e-03\n    * min lr: 1.0e-04\n    \n### NOTE: I normalized spectrograms per image\n```python\nimg = np.load(path)  # shape: (Hz, Time) = (400, 300)\neps = 1e-6\nimg_mean = img.mean(axis=(0, 1))\nimg = img - img_mean\nimg_std = img.std(axis=(0, 1))\nimg = img / (img_std + eps)\n```","metadata":{}},{"cell_type":"markdown","source":"# Prepare","metadata":{}},{"cell_type":"markdown","source":"## import","metadata":{}},{"cell_type":"code","source":"import sys\nimport os\nimport gc\nimport copy\nimport yaml\nimport random\nimport shutil\nfrom time import time\nimport typing as tp\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import StratifiedGroupKFold\n\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.optim import lr_scheduler\nfrom torch.cuda import amp\n\nimport timm\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-05-04T11:31:06.189603Z","iopub.execute_input":"2024-05-04T11:31:06.189863Z","iopub.status.idle":"2024-05-04T11:31:13.851238Z","shell.execute_reply.started":"2024-05-04T11:31:06.189839Z","shell.execute_reply":"2024-05-04T11:31:13.850102Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"","metadata":{"execution":{"iopub.status.busy":"2024-05-04T11:31:13.852967Z","iopub.execute_input":"2024-05-04T11:31:13.853440Z","iopub.status.idle":"2024-05-04T11:31:13.857651Z","shell.execute_reply.started":"2024-05-04T11:31:13.853412Z","shell.execute_reply":"2024-05-04T11:31:13.856630Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"ROOT = Path.cwd().parent\nINPUT = ROOT / \"input\"\nOUTPUT = ROOT / \"output\"\nSRC = ROOT / \"src\"\n\nDATA = INPUT / \"hms-harmful-brain-activity-classification\"\nTRAIN_SPEC = DATA / \"train_spectrograms\"\nTEST_SPEC = DATA / \"test_spectrograms\"\n\nTMP = ROOT / \"tmp\"\nTRAIN_SPEC_SPLIT = TMP / \"train_spectrograms_split\"\nTEST_SPEC_SPLIT = TMP / \"test_spectrograms_split\"\nTMP.mkdir(exist_ok=True)\nTRAIN_SPEC_SPLIT.mkdir(exist_ok=True)\nTEST_SPEC_SPLIT.mkdir(exist_ok=True)\n\n\nRANDAM_SEED = 1086\nCLASSES = [\"seizure_vote\", \"lpd_vote\", \"gpd_vote\", \"lrda_vote\", \"grda_vote\", \"other_vote\"]\nN_CLASSES = len(CLASSES)\nFOLDS = [0, 1, 2, 3, 4]\nN_FOLDS = len(FOLDS)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-05-04T11:31:19.346056Z","iopub.execute_input":"2024-05-04T11:31:19.347025Z","iopub.status.idle":"2024-05-04T11:31:19.354534Z","shell.execute_reply.started":"2024-05-04T11:31:19.346979Z","shell.execute_reply":"2024-05-04T11:31:19.353601Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Read Data, Split Folds, Split Spectrograms","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(DATA / \"train.csv\")\n\n# convert vote to probability\ntrain[CLASSES] /= train[CLASSES].sum(axis=1).values[:, None]\n\nprint(train.shape)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-05-04T11:31:21.488127Z","iopub.execute_input":"2024-05-04T11:31:21.488580Z","iopub.status.idle":"2024-05-04T11:31:21.769752Z","shell.execute_reply.started":"2024-05-04T11:31:21.488550Z","shell.execute_reply":"2024-05-04T11:31:21.768668Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"(106800, 15)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### NOTE: I used the **first** `spectrogram_sub_id` for each `spectrogram_id` in order to train model faster.","metadata":{"execution":{"iopub.execute_input":"2024-01-14T00:22:27.085297Z","iopub.status.busy":"2024-01-14T00:22:27.084935Z","iopub.status.idle":"2024-01-14T00:22:27.093145Z","shell.execute_reply":"2024-01-14T00:22:27.091394Z","shell.execute_reply.started":"2024-01-14T00:22:27.085268Z"}}},{"cell_type":"code","source":"train = train.groupby(\"spectrogram_id\").head(1).reset_index(drop=True)\nprint(train.shape)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-05-04T11:31:24.650337Z","iopub.execute_input":"2024-05-04T11:31:24.650979Z","iopub.status.idle":"2024-05-04T11:31:24.668868Z","shell.execute_reply.started":"2024-05-04T11:31:24.650946Z","shell.execute_reply":"2024-05-04T11:31:24.668039Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"(11138, 15)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### split folds","metadata":{}},{"cell_type":"code","source":"sgkf = StratifiedGroupKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDAM_SEED)\n\ntrain[\"fold\"] = -1\n\nfor fold_id, (_, val_idx) in enumerate(\n    sgkf.split(train, y=train[\"expert_consensus\"], groups=train[\"patient_id\"])\n):\n    train.loc[val_idx, \"fold\"] = fold_id","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-05-04T11:31:26.837462Z","iopub.execute_input":"2024-05-04T11:31:26.838183Z","iopub.status.idle":"2024-05-04T11:31:27.793975Z","shell.execute_reply.started":"2024-05-04T11:31:26.838148Z","shell.execute_reply":"2024-05-04T11:31:27.792972Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train.groupby(\"fold\")[CLASSES].sum()","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-05-04T11:31:29.897771Z","iopub.execute_input":"2024-05-04T11:31:29.898445Z","iopub.status.idle":"2024-05-04T11:31:29.920694Z","shell.execute_reply.started":"2024-05-04T11:31:29.898411Z","shell.execute_reply":"2024-05-04T11:31:29.919683Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"      seizure_vote    lpd_vote    gpd_vote   lrda_vote   grda_vote  \\\nfold                                                                 \n0       407.878970  240.847820  262.474513  142.304068  286.407590   \n1       360.427388  231.931854  193.738000  173.763906  333.566517   \n2       441.934721  328.255479  237.291923  163.192668  355.493987   \n3       425.685980  195.568155  182.017264  148.850582  259.828026   \n4       392.391708  234.916737  120.355588  129.112045  258.598367   \n\n       other_vote  \nfold               \n0      800.087038  \n1     1166.572336  \n2      926.831222  \n3      864.049993  \n4      873.625556  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>seizure_vote</th>\n      <th>lpd_vote</th>\n      <th>gpd_vote</th>\n      <th>lrda_vote</th>\n      <th>grda_vote</th>\n      <th>other_vote</th>\n    </tr>\n    <tr>\n      <th>fold</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>407.878970</td>\n      <td>240.847820</td>\n      <td>262.474513</td>\n      <td>142.304068</td>\n      <td>286.407590</td>\n      <td>800.087038</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>360.427388</td>\n      <td>231.931854</td>\n      <td>193.738000</td>\n      <td>173.763906</td>\n      <td>333.566517</td>\n      <td>1166.572336</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>441.934721</td>\n      <td>328.255479</td>\n      <td>237.291923</td>\n      <td>163.192668</td>\n      <td>355.493987</td>\n      <td>926.831222</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>425.685980</td>\n      <td>195.568155</td>\n      <td>182.017264</td>\n      <td>148.850582</td>\n      <td>259.828026</td>\n      <td>864.049993</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>392.391708</td>\n      <td>234.916737</td>\n      <td>120.355588</td>\n      <td>129.112045</td>\n      <td>258.598367</td>\n      <td>873.625556</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### split sepectogram files","metadata":{}},{"cell_type":"code","source":"for spec_id, df in tqdm(train.groupby(\"spectrogram_id\")):\n    spec = pd.read_parquet(TRAIN_SPEC / f\"{spec_id}.parquet\")\n    \n    spec_arr = spec.fillna(0).values[:, 1:].T.astype(\"float32\")  # (Hz, Time) = (400, 300)\n    \n    for spec_offset, label_id in df[\n        [\"spectrogram_label_offset_seconds\", \"label_id\"]\n    ].astype(int).values:\n        spec_offset = spec_offset // 2\n        split_spec_arr = spec_arr[:, spec_offset: spec_offset + 300]\n        np.save(TRAIN_SPEC_SPLIT / f\"{label_id}.npy\" , split_spec_arr)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-05-04T11:31:33.326184Z","iopub.execute_input":"2024-05-04T11:31:33.326792Z","iopub.status.idle":"2024-05-04T11:40:27.047075Z","shell.execute_reply.started":"2024-05-04T11:31:33.326765Z","shell.execute_reply":"2024-05-04T11:40:27.046095Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/11138 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6850f9e3db304a0a8887474bbc077f10"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Difinition, Model, Dataset, Metric","metadata":{}},{"cell_type":"markdown","source":"### model","metadata":{}},{"cell_type":"code","source":"class HMSHBACSpecModel(nn.Module):\n\n    def __init__(\n            self,\n            model_name: str,\n            pretrained: bool,\n            in_channels: int,\n            num_classes: int,\n        ):\n        super().__init__()\n        self.model = timm.create_model(\n            model_name=model_name, pretrained=pretrained,\n            num_classes=num_classes, in_chans=in_channels)\n\n    def forward(self, x):\n        h = self.model(x)      \n\n        return h","metadata":{"_kg_hide-input":false,"tags":[],"execution":{"iopub.status.busy":"2024-05-04T11:40:33.387335Z","iopub.execute_input":"2024-05-04T11:40:33.388215Z","iopub.status.idle":"2024-05-04T11:40:33.394384Z","shell.execute_reply.started":"2024-05-04T11:40:33.388183Z","shell.execute_reply":"2024-05-04T11:40:33.393408Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### dataset","metadata":{}},{"cell_type":"code","source":"FilePath = tp.Union[str, Path]\nLabel = tp.Union[int, float, np.ndarray]\n\nclass HMSHBACSpecDataset(torch.utils.data.Dataset):\n\n    def __init__(\n        self,\n        image_paths: tp.Sequence[FilePath],\n        labels: tp.Sequence[Label],\n        transform: A.Compose,\n    ):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, index: int):\n        img_path = self.image_paths[index]\n        label = self.labels[index]\n\n        img = np.load(img_path)  # shape: (Hz, Time) = (400, 300)\n        \n        # log transform\n        img = np.clip(img,np.exp(-4), np.exp(8))\n        img = np.log(img)\n        \n        # normalize per image\n        eps = 1e-6\n        img_mean = img.mean(axis=(0, 1))\n        img = img - img_mean\n        img_std = img.std(axis=(0, 1))\n        img = img / (img_std + eps)\n\n        img = img[..., None] # shape: (Hz, Time) -> (Hz, Time, Channel)\n        img = self._apply_transform(img)\n\n        return {\"data\": img, \"target\": label}\n\n    def _apply_transform(self, img: np.ndarray):\n        \"\"\"apply transform to image and mask\"\"\"\n        transformed = self.transform(image=img)\n        img = transformed[\"image\"]\n        return img","metadata":{"_kg_hide-input":false,"tags":[],"execution":{"iopub.status.busy":"2024-05-04T11:40:36.886017Z","iopub.execute_input":"2024-05-04T11:40:36.886943Z","iopub.status.idle":"2024-05-04T11:40:36.897081Z","shell.execute_reply.started":"2024-05-04T11:40:36.886910Z","shell.execute_reply":"2024-05-04T11:40:36.896189Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### loss","metadata":{}},{"cell_type":"code","source":"class KLDivLossWithLogits(nn.KLDivLoss):\n\n    def __init__(self):\n        super().__init__(reduction=\"batchmean\")\n\n    def forward(self, y, t):\n        y = nn.functional.log_softmax(y,  dim=1)\n        loss = super().forward(y, t)\n\n        return loss\n\n\nclass KLDivLossWithLogitsForVal(nn.KLDivLoss):\n    \n    def __init__(self):\n        \"\"\"\"\"\"\n        super().__init__(reduction=\"batchmean\")\n        self.log_prob_list  = []\n        self.label_list = []\n\n    def forward(self, y, t):\n        y = nn.functional.log_softmax(y, dim=1)\n        self.log_prob_list.append(y.numpy())\n        self.label_list.append(t.numpy())\n        \n    def compute(self):\n        log_prob = np.concatenate(self.log_prob_list, axis=0)\n        label = np.concatenate(self.label_list, axis=0)\n        final_metric = super().forward(\n            torch.from_numpy(log_prob),\n            torch.from_numpy(label)\n        ).item()\n        self.log_prob_list = []\n        self.label_list = []\n        \n        return final_metric","metadata":{"_kg_hide-input":false,"tags":[],"execution":{"iopub.status.busy":"2024-05-04T11:40:39.396826Z","iopub.execute_input":"2024-05-04T11:40:39.397281Z","iopub.status.idle":"2024-05-04T11:40:39.407080Z","shell.execute_reply.started":"2024-05-04T11:40:39.397245Z","shell.execute_reply":"2024-05-04T11:40:39.406091Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"markdown","source":"## Config","metadata":{}},{"cell_type":"code","source":"class CFG:\n    model_name = \"resnet34d\"\n    img_size = 512\n    max_epoch = 9\n    batch_size = 32\n    lr = 1.0e-03\n    weight_decay = 1.0e-02\n    es_patience =  5\n    seed = 1086\n    deterministic = True\n    enable_amp = True\n    device = \"cuda\"","metadata":{"execution":{"iopub.status.busy":"2024-05-04T11:40:41.847572Z","iopub.execute_input":"2024-05-04T11:40:41.848386Z","iopub.status.idle":"2024-05-04T11:40:41.853517Z","shell.execute_reply.started":"2024-05-04T11:40:41.848353Z","shell.execute_reply":"2024-05-04T11:40:41.852623Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Functions for training","metadata":{}},{"cell_type":"code","source":"def set_random_seed(seed: int = 42, deterministic: bool = False):\n    \"\"\"Set seeds\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)  # type: ignore\n    torch.backends.cudnn.deterministic = deterministic  # type: ignore\n    \ndef to_device(\n    tensors: tp.Union[tp.Tuple[torch.Tensor], tp.Dict[str, torch.Tensor]],\n    device: torch.device, *args, **kwargs\n):\n    if isinstance(tensors, tuple):\n        return (t.to(device, *args, **kwargs) for t in tensors)\n    elif isinstance(tensors, dict):\n        return {\n            k: t.to(device, *args, **kwargs) for k, t in tensors.items()}\n    else:\n        return tensors.to(device, *args, **kwargs)","metadata":{"_kg_hide-input":false,"tags":[],"execution":{"iopub.status.busy":"2024-05-04T11:40:43.442910Z","iopub.execute_input":"2024-05-04T11:40:43.443742Z","iopub.status.idle":"2024-05-04T11:40:43.451871Z","shell.execute_reply.started":"2024-05-04T11:40:43.443693Z","shell.execute_reply":"2024-05-04T11:40:43.450855Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def get_path_label(val_fold, train_all: pd.DataFrame):\n    \"\"\"Get file path and target info.\"\"\"\n    \n    train_idx = train_all[train_all[\"fold\"] != val_fold].index.values\n    val_idx   = train_all[train_all[\"fold\"] == val_fold].index.values\n    img_paths = []\n    labels = train_all[CLASSES].values\n    for label_id in train_all[\"label_id\"].values:\n        img_path = TRAIN_SPEC_SPLIT / f\"{label_id}.npy\"\n        img_paths.append(img_path)\n\n    train_data = {\n        \"image_paths\": [img_paths[idx] for idx in train_idx],\n        \"labels\": [labels[idx].astype(\"float32\") for idx in train_idx]}\n\n    val_data = {\n        \"image_paths\": [img_paths[idx] for idx in val_idx],\n        \"labels\": [labels[idx].astype(\"float32\") for idx in val_idx]}\n    \n    return train_data, val_data, train_idx, val_idx\n\n\ndef get_transforms(CFG):\n    train_transform = A.Compose([\n        A.Resize(p=1.0, height=CFG.img_size, width=CFG.img_size),\n        ToTensorV2(p=1.0)\n    ])\n    val_transform = A.Compose([\n        A.Resize(p=1.0, height=CFG.img_size, width=CFG.img_size),\n        ToTensorV2(p=1.0)\n    ])\n    return train_transform, val_transform","metadata":{"_kg_hide-input":false,"tags":[],"execution":{"iopub.status.busy":"2024-05-04T11:40:44.740136Z","iopub.execute_input":"2024-05-04T11:40:44.740999Z","iopub.status.idle":"2024-05-04T11:40:44.749805Z","shell.execute_reply.started":"2024-05-04T11:40:44.740967Z","shell.execute_reply":"2024-05-04T11:40:44.748794Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def train_one_fold(CFG, val_fold, train_all, output_path):\n    \"\"\"Main\"\"\"\n    torch.backends.cudnn.benchmark = True\n    set_random_seed(CFG.seed, deterministic=CFG.deterministic)\n    device = torch.device(CFG.device)\n    \n    train_path_label, val_path_label, _, _ = get_path_label(val_fold, train_all)\n    train_transform, val_transform = get_transforms(CFG)\n    \n    train_dataset = HMSHBACSpecDataset(**train_path_label, transform=train_transform)\n    val_dataset = HMSHBACSpecDataset(**val_path_label, transform=val_transform)\n    \n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=CFG.batch_size, num_workers=4, shuffle=True, drop_last=True)\n    val_loader = torch.utils.data.DataLoader(\n        val_dataset, batch_size=CFG.batch_size, num_workers=4, shuffle=False, drop_last=False)\n    \n    model = HMSHBACSpecModel(\n        model_name=CFG.model_name, pretrained=True, num_classes=6, in_channels=1)\n    model.to(device)\n    \n    optimizer = optim.AdamW(params=model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\n    scheduler = lr_scheduler.OneCycleLR(\n        optimizer=optimizer, epochs=CFG.max_epoch,\n        pct_start=0.0, steps_per_epoch=len(train_loader),\n        max_lr=CFG.lr, div_factor=25, final_div_factor=4.0e-01\n    )\n    \n    loss_func = KLDivLossWithLogits()\n    loss_func.to(device)\n    loss_func_val = KLDivLossWithLogitsForVal()\n    \n    use_amp = CFG.enable_amp\n    scaler = amp.GradScaler(enabled=use_amp)\n    \n    best_val_loss = 1.0e+09\n    best_epoch = 0\n    train_loss = 0\n    \n    for epoch in range(1, CFG.max_epoch + 1):\n        epoch_start = time()\n        model.train()\n        for batch in train_loader:\n            batch = to_device(batch, device)\n            x, t = batch[\"data\"], batch[\"target\"]\n                \n            optimizer.zero_grad()\n            with amp.autocast(use_amp):\n                y = model(x)\n                loss = loss_func(y, t)\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            scheduler.step()\n            train_loss += loss.item()\n            \n        train_loss /= len(train_loader)\n            \n        model.eval()\n        for batch in val_loader:\n            x, t = batch[\"data\"], batch[\"target\"]\n            x = to_device(x, device)\n            with torch.no_grad(), amp.autocast(use_amp):\n                y = model(x)\n            y = y.detach().cpu().to(torch.float32)\n            loss_func_val(y, t)\n        val_loss = loss_func_val.compute()        \n        if val_loss < best_val_loss:\n            best_epoch = epoch\n            best_val_loss = val_loss\n            # print(\"save model\")\n            torch.save(model.state_dict(), str(output_path / f'snapshot_epoch_{epoch}.pth'))\n        \n        elapsed_time = time() - epoch_start\n        print(\n            f\"[epoch {epoch}] train loss: {train_loss: .6f}, val loss: {val_loss: .6f}, elapsed_time: {elapsed_time: .3f}\")\n        \n        if epoch - best_epoch > CFG.es_patience:\n            print(\"Early Stopping!\")\n            break\n            \n        train_loss = 0\n            \n    return val_fold, best_epoch, best_val_loss","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-05-04T11:40:45.510920Z","iopub.execute_input":"2024-05-04T11:40:45.511560Z","iopub.status.idle":"2024-05-04T11:40:45.527692Z","shell.execute_reply.started":"2024-05-04T11:40:45.511527Z","shell.execute_reply":"2024-05-04T11:40:45.526852Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Run Training","metadata":{}},{"cell_type":"code","source":"score_list = []\nfor fold_id in FOLDS:\n    output_path = Path(f\"fold{fold_id}\")\n    output_path.mkdir(exist_ok=True)\n    print(f\"[fold{fold_id}]\")\n    score_list.append(train_one_fold(CFG, fold_id, train, output_path))","metadata":{"execution":{"iopub.status.busy":"2024-05-04T11:40:47.245479Z","iopub.execute_input":"2024-05-04T11:40:47.246344Z","iopub.status.idle":"2024-05-04T13:13:36.154997Z","shell.execute_reply.started":"2024-05-04T11:40:47.246310Z","shell.execute_reply":"2024-05-04T13:13:36.153754Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"[fold0]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/87.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c57a40ffbd847778d6a9d55d6f2ec74"}},"metadata":{}},{"name":"stdout","text":"[epoch 1] train loss:  0.882599, val loss:  0.851408, elapsed_time:  136.889\n[epoch 2] train loss:  0.698157, val loss:  0.820204, elapsed_time:  124.286\n[epoch 3] train loss:  0.608675, val loss:  0.744616, elapsed_time:  123.949\n[epoch 4] train loss:  0.528086, val loss:  0.753391, elapsed_time:  124.226\n[epoch 5] train loss:  0.427589, val loss:  0.781515, elapsed_time:  124.063\n[epoch 6] train loss:  0.318129, val loss:  0.756746, elapsed_time:  124.139\n[epoch 7] train loss:  0.224556, val loss:  0.803439, elapsed_time:  124.015\n[epoch 8] train loss:  0.153463, val loss:  0.786738, elapsed_time:  123.968\n[epoch 9] train loss:  0.116854, val loss:  0.824401, elapsed_time:  124.217\nEarly Stopping!\n[fold1]\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n","output_type":"stream"},{"name":"stdout","text":"[epoch 1] train loss:  0.904280, val loss:  0.938235, elapsed_time:  121.299\n[epoch 2] train loss:  0.720165, val loss:  0.900330, elapsed_time:  121.501\n[epoch 3] train loss:  0.638118, val loss:  0.706957, elapsed_time:  121.144\n[epoch 4] train loss:  0.552333, val loss:  0.737890, elapsed_time:  121.325\n[epoch 5] train loss:  0.457910, val loss:  0.774971, elapsed_time:  121.119\n[epoch 6] train loss:  0.350029, val loss:  0.703400, elapsed_time:  121.281\n[epoch 7] train loss:  0.252007, val loss:  0.714577, elapsed_time:  121.121\n[epoch 8] train loss:  0.175336, val loss:  0.723147, elapsed_time:  121.234\n[epoch 9] train loss:  0.132709, val loss:  0.776623, elapsed_time:  121.364\n[fold2]\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n","output_type":"stream"},{"name":"stdout","text":"[epoch 1] train loss:  0.898212, val loss:  0.886631, elapsed_time:  122.104\n[epoch 2] train loss:  0.710584, val loss:  0.811041, elapsed_time:  121.316\n[epoch 3] train loss:  0.627898, val loss:  0.830818, elapsed_time:  121.113\n[epoch 4] train loss:  0.543182, val loss:  0.865721, elapsed_time:  120.968\n[epoch 5] train loss:  0.456368, val loss:  0.702043, elapsed_time:  121.464\n[epoch 6] train loss:  0.351186, val loss:  0.767019, elapsed_time:  121.270\n[epoch 7] train loss:  0.249169, val loss:  0.813521, elapsed_time:  121.358\n[epoch 8] train loss:  0.169040, val loss:  0.833246, elapsed_time:  121.285\n[epoch 9] train loss:  0.128260, val loss:  0.853226, elapsed_time:  121.222\n[fold3]\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n","output_type":"stream"},{"name":"stdout","text":"[epoch 1] train loss:  0.908737, val loss:  0.935427, elapsed_time:  124.786\n[epoch 2] train loss:  0.706002, val loss:  0.724434, elapsed_time:  124.586\n[epoch 3] train loss:  0.618819, val loss:  0.730945, elapsed_time:  124.886\n[epoch 4] train loss:  0.535117, val loss:  0.739224, elapsed_time:  124.880\n[epoch 5] train loss:  0.452238, val loss:  0.709114, elapsed_time:  124.696\n[epoch 6] train loss:  0.344210, val loss:  0.767912, elapsed_time:  124.793\n[epoch 7] train loss:  0.243417, val loss:  0.813730, elapsed_time:  124.765\n[epoch 8] train loss:  0.166547, val loss:  0.848325, elapsed_time:  124.783\n[epoch 9] train loss:  0.124330, val loss:  0.883598, elapsed_time:  124.695\n[fold4]\n[epoch 1] train loss:  0.892550, val loss:  1.235761, elapsed_time:  126.615\n[epoch 2] train loss:  0.702810, val loss:  0.771075, elapsed_time:  125.348\n[epoch 3] train loss:  0.614388, val loss:  0.837626, elapsed_time:  125.135\n[epoch 4] train loss:  0.535080, val loss:  0.723097, elapsed_time:  125.182\n[epoch 5] train loss:  0.438436, val loss:  0.857026, elapsed_time:  125.151\n[epoch 6] train loss:  0.325680, val loss:  0.809226, elapsed_time:  125.377\n[epoch 7] train loss:  0.229884, val loss:  0.833756, elapsed_time:  125.043\n[epoch 8] train loss:  0.162821, val loss:  0.863633, elapsed_time:  125.170\n[epoch 9] train loss:  0.121084, val loss:  0.889576, elapsed_time:  125.182\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Inference Out Of Fold","metadata":{}},{"cell_type":"markdown","source":"## Copy best models","metadata":{}},{"cell_type":"code","source":"print(score_list)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T13:14:41.043503Z","iopub.execute_input":"2024-05-04T13:14:41.044260Z","iopub.status.idle":"2024-05-04T13:14:41.049501Z","shell.execute_reply.started":"2024-05-04T13:14:41.044227Z","shell.execute_reply":"2024-05-04T13:14:41.048589Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"[(0, 3, 0.7446164488792419), (1, 6, 0.703400194644928), (2, 5, 0.7020434141159058), (3, 5, 0.7091140747070312), (4, 4, 0.7230966091156006)]\n","output_type":"stream"}]},{"cell_type":"code","source":"best_log_list = []\nfor (fold_id, best_epoch, _) in score_list:\n    \n    exp_dir_path = Path(f\"fold{fold_id}\")\n    best_model_path = exp_dir_path / f\"snapshot_epoch_{best_epoch}.pth\"\n    copy_to = f\"./best_model_fold{fold_id}.pth\"\n    shutil.copy(best_model_path, copy_to)\n    \n    for p in exp_dir_path.glob(\"*.pth\"):\n        p.unlink()","metadata":{"execution":{"iopub.status.busy":"2024-05-04T13:14:45.498586Z","iopub.execute_input":"2024-05-04T13:14:45.499208Z","iopub.status.idle":"2024-05-04T13:14:46.086157Z","shell.execute_reply.started":"2024-05-04T13:14:45.499176Z","shell.execute_reply":"2024-05-04T13:14:46.085210Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## Inference OOF","metadata":{}},{"cell_type":"code","source":"def run_inference_loop(model, loader, device):\n    model.to(device)\n    model.eval()\n    pred_list = []\n    with torch.no_grad():\n        for batch in tqdm(loader):\n            x = to_device(batch[\"data\"], device)\n            y = model(x)\n            pred_list.append(y.softmax(dim=1).detach().cpu().numpy())\n        \n    pred_arr = np.concatenate(pred_list)\n    del pred_list\n    return pred_arr","metadata":{"execution":{"iopub.status.busy":"2024-05-04T13:14:49.717237Z","iopub.execute_input":"2024-05-04T13:14:49.717859Z","iopub.status.idle":"2024-05-04T13:14:49.724153Z","shell.execute_reply.started":"2024-05-04T13:14:49.717827Z","shell.execute_reply":"2024-05-04T13:14:49.723267Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"label_arr = train[CLASSES].values\noof_pred_arr = np.zeros((len(train), N_CLASSES))\nscore_list = []\n\nfor fold_id in range(N_FOLDS):\n    print(f\"\\n[fold {fold_id}]\")\n    device = torch.device(CFG.device)\n\n    # # get_dataloader\n    _, val_path_label, _, val_idx = get_path_label(fold_id, train)\n    _, val_transform = get_transforms(CFG)\n    val_dataset = HMSHBACSpecDataset(**val_path_label, transform=val_transform)\n    val_loader = torch.utils.data.DataLoader(\n        val_dataset, batch_size=CFG.batch_size, num_workers=4, shuffle=False, drop_last=False)\n    \n    # # get model\n    model_path = f\"./best_model_fold{fold_id}.pth\"\n    model = HMSHBACSpecModel(\n        model_name=CFG.model_name, pretrained=False, num_classes=6, in_channels=1)\n    model.load_state_dict(torch.load(model_path, map_location=device))\n    \n    # # inference\n    val_pred = run_inference_loop(model, val_loader, device)\n    oof_pred_arr[val_idx] = val_pred\n    \n    del val_idx, val_path_label\n    del model, val_loader\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-05-04T13:14:53.651531Z","iopub.execute_input":"2024-05-04T13:14:53.652415Z","iopub.status.idle":"2024-05-04T13:15:50.253082Z","shell.execute_reply.started":"2024-05-04T13:14:53.652377Z","shell.execute_reply":"2024-05-04T13:15:50.252212Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"\n[fold 0]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/67 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c45716c492414afc909175a94d33d854"}},"metadata":{}},{"name":"stdout","text":"\n[fold 1]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/77 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88b7e941181742f8a9a6a92b8eb50e0e"}},"metadata":{}},{"name":"stdout","text":"\n[fold 2]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/77 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"152a5395f84a49bcae002afb33645817"}},"metadata":{}},{"name":"stdout","text":"\n[fold 3]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/65 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"473b085681fc4a3d97391c4b05cf2065"}},"metadata":{}},{"name":"stdout","text":"\n[fold 4]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd1f2164169c41d98ffc101b9832ef72"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Calculate OOF score","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/input/kaggle-kl-div')\nfrom kaggle_kl_div import score\n\ntrue = train[[\"label_id\"] + CLASSES].copy()\n\noof = pd.DataFrame(oof_pred_arr, columns=CLASSES)\noof.insert(0, \"label_id\", train[\"label_id\"])\n\ncv_score = score(solution=true, submission=oof, row_id_column_name='label_id')\nprint('CV Score KL-Div for ResNet34d',cv_score)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T13:15:50.254905Z","iopub.execute_input":"2024-05-04T13:15:50.255211Z","iopub.status.idle":"2024-05-04T13:15:50.317059Z","shell.execute_reply.started":"2024-05-04T13:15:50.255185Z","shell.execute_reply":"2024-05-04T13:15:50.316256Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"CV Score KL-Div for ResNet34d 0.715760646315781\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# EOF","metadata":{}}]}